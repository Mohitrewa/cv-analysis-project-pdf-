{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# # Install Libraries and dependencies"
      ],
      "metadata": {
        "id": "4sDW8fYuEhpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gtts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyZFSJV49coD",
        "outputId": "f9d2bbb4-d890-4f80-f2d1-775d7e059f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.31.0)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.2.2)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zalPekXALw_c",
        "outputId": "ee27b470-8600-4058-b1c6-7654c1b4d86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5uONEOfjM4c",
        "outputId": "30ea070f-07e4-4282-a693-cadb06815de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import PyPDF2\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "vd5sHR_eWkn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "B0Qm0aEO9WIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract__section(pdf_path):\n",
        "    # experience_section = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "        for page_num in range(num_pages):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # match = re.search(r'Experience\\s*:?([\\s\\S]+?)(?:(?:Education)|(?:Skills)|(?:Languages)|(?:Projects)|(?:Certifications)|(?:References)|\\Z)', text, re.IGNORECASE)\n",
        "            # if match:\n",
        "            #     experience_section += match.group(1).strip() + \"\\n\"\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "4tecTITkLAEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_exp_sction(text):\n",
        "  experience_section = \"\"\n",
        "  match = re.search(r'Experience\\s*:?([\\s\\S]+?)(?:(?:Education)|(?:Skills)|(?:Languages)|(?:Projects)|(?:Certifications)|(?:References)|\\Z)', text, re.IGNORECASE)\n",
        "  if match:\n",
        "    experience_section += match.group(1).strip() + \"\\n\"\n",
        "  return experience_section"
      ],
      "metadata": {
        "id": "GKZ8NrvHSLZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_total_experience(exp_section):\n",
        "  date_pattern = r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep.(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{4}\\s*–\\s*(?:Present|\\w+\\s+\\d{4})'\n",
        "  years = re.findall(date_pattern, exp_section)\n",
        "  return years"
      ],
      "metadata": {
        "id": "7dT1xWA026Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dates(get_years_list):\n",
        "\n",
        "  get_current_date = datetime.now()\n",
        "  # get_current_date = datetime.strptime(get_current_date, \"%B %Y\")\n",
        "  # print(datetime.strptime(get_current_date, \"%B %Y\"))\n",
        "\n",
        "  modified_list = []\n",
        "\n",
        "  for item in get_years_list:\n",
        "      date_parts = item.split(' – ')\n",
        "      # print(date_parts)\n",
        "      modified_list.append(date_parts)\n",
        "\n",
        "  # print(modified_list[0][0].replace('.',''))\n",
        "\n",
        "  for dt_range in range(len(modified_list)):\n",
        "    for each_dt in range(len(modified_list[dt_range])):\n",
        "      get_date = modified_list[dt_range][each_dt].replace('.','')\n",
        "      if get_date == 'Present' or get_date == 'Current':\n",
        "        modified_list[dt_range][each_dt] = get_current_date.date()\n",
        "      else:\n",
        "        try:\n",
        "          datetime_obj = datetime.strptime(get_date, \"%B %Y\")\n",
        "        except ValueError:\n",
        "          try:\n",
        "            datetime_obj = datetime.strptime(get_date, \"%b %Y\")\n",
        "          except ValueError:\n",
        "            print(\"Error: Unable to parse date string:\", get_date)\n",
        "            continue\n",
        "        modified_list[dt_range][each_dt] = datetime_obj.date()\n",
        "\n",
        "      # print(get_date)\n",
        "  # print(modified_list)\n",
        "  return modified_list"
      ],
      "metadata": {
        "id": "J9OTwe4a2yYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_total_years(formatted_years_list):\n",
        "  prev_emp_start = datetime.now().date()\n",
        "  prev_emp_end = datetime.now().date()\n",
        "  total_days = 0\n",
        "  total_months = 0\n",
        "  total_years = 0\n",
        "\n",
        "  for dt_range in formatted_years_list:\n",
        "    get_diff = dt_range[1] - dt_range[0]\n",
        "    # print('\\n')\n",
        "    # print(dt_range[0],dt_range[1])\n",
        "    # print(prev_emp_start,prev_emp_end)\n",
        "    # if dt_range[1] <= prev_emp_start:\n",
        "    #   get_diff = dt_range[1] - dt_range[0]\n",
        "    # else:\n",
        "    #   get_diff = prev_emp_start - dt_range[0]\n",
        "\n",
        "    if get_diff != 0 and get_diff.days > 0:\n",
        "      total_days += get_diff.days\n",
        "\n",
        "    prev_emp_start = dt_range[0]\n",
        "    prev_emp_end = dt_range[1]\n",
        "\n",
        "  total_years_calc = total_days / 365\n",
        "  total_years = int(total_years_calc)\n",
        "  total_months = int((total_years_calc - total_years) * 12)\n",
        "\n",
        "\n",
        "  return [total_years, total_months]"
      ],
      "metadata": {
        "id": "VPIu_FWKH71x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_exp_year_from_pdf(pdf_path):\n",
        "  extract_text = extract__section(pdf_path)\n",
        "\n",
        "  experience_section = get_exp_sction(extract_text)\n",
        "\n",
        "  years_list = extract_total_experience(experience_section)\n",
        "  formatted_years_list = format_dates(years_list)\n",
        "  total_years_worked = calculate_total_years(formatted_years_list)\n",
        "\n",
        "  return total_years_worked"
      ],
      "metadata": {
        "id": "sM95jKgzWHDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_skill_section(extract_text):\n",
        "  skills_section_pattern = r'Skills\\s*:?([\\s\\S]+?)(?:(?:Experience)|(?:Education)|(?:Languages)|(?:Projects)|(?:Certifications)|(?:References)|\\Z)'\n",
        "  skills_section_match = re.search(skills_section_pattern, extract_text, re.IGNORECASE)\n",
        "  skills_section = None\n",
        "  if skills_section_match:\n",
        "      skills_section = skills_section_match.group(1).strip()\n",
        "      # print(\"Skills Section:\")\n",
        "      # print(skills_section)\n",
        "  else:\n",
        "      print(\"Skills section not found.\")\n",
        "\n",
        "  return skills_section\n"
      ],
      "metadata": {
        "id": "ldyAURRGdxPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_skill_dict(get_all_skills):\n",
        "  all_skills = get_all_skills.split('\\n')\n",
        "  skill_list = {}\n",
        "  for s1 in all_skills:\n",
        "    new_s1 = s1.split(':')\n",
        "    for n1 in new_s1[1].split(','):\n",
        "      skill_list[n1] = 0\n",
        "  # print(skill_list)\n",
        "\n",
        "  return skill_list"
      ],
      "metadata": {
        "id": "pS4RdYgEeXKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_skill_usage(skill_dict, extract_text):\n",
        "  for each_skill in skill_dict:\n",
        "    skill_search = extract_text.count(each_skill)\n",
        "    skill_dict[each_skill] = skill_search\n",
        "  # print(skill_list)\n",
        "  return skill_dict"
      ],
      "metadata": {
        "id": "2MKGzdigfHIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_skills_section_from_pdf(pdf_path):\n",
        "  extract_text = extract__section(pdf_path)\n",
        "  get_all_skills = get_skill_section(extract_text)\n",
        "  skill_dict = create_skill_dict(get_all_skills)\n",
        "  updated_skill_dict = find_skill_usage(skill_dict, extract_text)\n",
        "\n",
        "  return updated_skill_dict"
      ],
      "metadata": {
        "id": "GyUXnFI3KYuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input"
      ],
      "metadata": {
        "id": "ur0upUF3Cf8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"Jake_s_Resume__9_.pdf\"\n",
        "# pdf_path = \"jakes-resume.pdf\"\n",
        "\n",
        "\n",
        "candidate_exp_years, candidate_exp_months = get_exp_year_from_pdf(pdf_path)\n",
        "get_years_of_exp = str(candidate_exp_years) + 'years '+ str(candidate_exp_months)+ 'months '\n",
        "# print(candidate_exp_years, 'years ', candidate_exp_months, 'months ')\n",
        "print(\"Years of Experience: \", get_years_of_exp)\n",
        "\n",
        "get_skills = get_skills_section_from_pdf(pdf_path)\n",
        "\n",
        "print('\\nAreas of Expertise:', get_skills.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uHqpvIB3bTs",
        "outputId": "ba45ff2b-fb4b-4468-803f-d5302f171074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Years of Experience:  3years 2months \n",
            "\n",
            "Areas of Expertise: dict_keys([' Java', ' Python', ' C/C++', ' SQL (Postgres)', ' JavaScript', ' HTML/CSS', ' R', ' React', ' Node.js', ' Flask', ' JUnit', ' WordPress', ' Material-UI', ' FastAPI', ' Git', ' Docker', ' TravisCI', ' Google Cloud Platform', ' VS Code', ' Visual Studio', ' PyCharm', ' IntelliJ', ' Eclipse', ' pandas', ' NumPy', ' Matplotlib'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat"
      ],
      "metadata": {
        "id": "4bXfcEDQCj94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_professional_words(term):\n",
        "    professional_words = set()\n",
        "    for synset in wordnet.synsets(term):\n",
        "        for lemma in synset.lemmas():\n",
        "            synonym = lemma.name()\n",
        "            professional_words.add(synonym)\n",
        "            for hypernym in synset.hypernyms():\n",
        "                professional_words.add(hypernym.lemma_names()[0])\n",
        "            for hyponym in synset.hyponyms():\n",
        "                professional_words.add(hyponym.lemma_names()[0])\n",
        "    return professional_words"
      ],
      "metadata": {
        "id": "DUT3gXbk9dpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_professional_vocab_exp():\n",
        "  expereince_terms = [\"professional\", \"experience\", \"career\", \"occupation\", \"work\", \"job\", \"employment\", \"vocation\", \"involve\"]\n",
        "  vocab_exp = set()\n",
        "  for term in expereince_terms:\n",
        "      vocab_exp.update(get_professional_words(term))\n",
        "  # print(vocab_exp)\n",
        "  return vocab_exp"
      ],
      "metadata": {
        "id": "5zgJzsTo9g-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_professional_vocab_skill():\n",
        "  skill_terms = ['skill', 'expertise', 'knowledge']\n",
        "  vocab_skill = set()\n",
        "  for term in skill_terms:\n",
        "      vocab_skill.update(get_professional_words(term))\n",
        "  # print(vocab_skill)\n",
        "  return vocab_skill"
      ],
      "metadata": {
        "id": "BmPzzMwX9zUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_required_info(required_list):\n",
        "  professional_vocab_exp = get_professional_vocab_exp()\n",
        "  professional_vocab_skill = get_professional_vocab_skill()\n",
        "\n",
        "  for word in required_list:\n",
        "    if word in professional_vocab_exp and word in professional_vocab_skill:\n",
        "      return [get_years_of_exp, get_skills]\n",
        "    elif word in professional_vocab_exp:\n",
        "      return [get_years_of_exp, None]\n",
        "    elif word in professional_vocab_skill:\n",
        "      return [None, get_skills]\n",
        "    else:\n",
        "      continue\n",
        "  return [None, None]"
      ],
      "metadata": {
        "id": "wXibsr-3-oBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    preprocessed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    return preprocessed_tokens\n",
        "\n",
        "\n",
        "def provide_answer(input_question):\n",
        "    preprocessed_question = preprocess_text(input_question)\n",
        "\n",
        "    lemmatized_words = []\n",
        "    for word in preprocessed_question:\n",
        "        pos_tag = nltk.pos_tag([word])[0][1][0].lower()\n",
        "        if pos_tag in ['a', 'r', 'n', 'v']:\n",
        "            lemma = lemmatizer.lemmatize(word, pos=pos_tag)\n",
        "        else:\n",
        "            lemma = lemmatizer.lemmatize(word)\n",
        "        lemmatized_words.append(lemma)\n",
        "\n",
        "    # print(lemmatized_words)\n",
        "\n",
        "    return check_required_info(lemmatized_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "sAxqg7lR-a4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat: Ask Question"
      ],
      "metadata": {
        "id": "yPHx75CDDIRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = input(\"Please enter your question: \")"
      ],
      "metadata": {
        "id": "Uyt5-6hZC9lf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e8aa88-9a68-4640-c0b8-aeace05de902"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your question: How many years of experience?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# input_question = \"What is the extent of your professional experience in your current domain?\"\n",
        "\n",
        "\n",
        "answer_exp, answer_skill = provide_answer(question)\n",
        "\n",
        "print(question)\n",
        "print(\"Answer:\")\n",
        "\n",
        "if not answer_exp and  not answer_skill:\n",
        "  print(\"Sorry, couldn't extract the requested information from the CV.\")\n",
        "\n",
        "if answer_exp:\n",
        "  print(answer_exp)\n",
        "\n",
        "if answer_skill:\n",
        "  print('Expertise')\n",
        "  for k_skill in answer_skill:\n",
        "    if answer_skill[k_skill] > 1:\n",
        "      print(k_skill)\n",
        "  print('Familiar')\n",
        "  for k_skill in answer_skill:\n",
        "    if answer_skill[k_skill] <= 1:\n",
        "      print(k_skill)\n",
        "\n"
      ],
      "metadata": {
        "id": "9AZJPaaW-axG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a336d2b3-dec7-4350-be62-60035dd4d92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How many years of experience?\n",
            "Answer:\n",
            "3years 2months \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3FryK_36CrJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "import os"
      ],
      "metadata": {
        "id": "Q1l9j7Nd-EME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_speech(text, lang='en'):\n",
        "\n",
        "    tts = gTTS(text=text, lang=lang)\n",
        "\n",
        "\n",
        "    tts.save(\"output.mp3\")\n",
        "\n",
        "\n",
        "    os.system(\"start output.mp3\")\n",
        "\n",
        "if not answer_exp and  not answer_skill:\n",
        "  text_to_speech(\"Sorry, couldn't extract the requested information from the CV.\")\n",
        "\n",
        "if answer_exp:\n",
        "  text_to_speech(answer_exp)\n",
        "\n",
        "if answer_skill:\n",
        "  text_to_speech(answer_skill)"
      ],
      "metadata": {
        "id": "m7tynMaHPbqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def text_to_speech(text, lang='en'):\n",
        "\n",
        "#     tts = gTTS(text=text, lang=lang)\n",
        "\n",
        "\n",
        "#     tts.save(\"output.mp3\")\n",
        "\n",
        "\n",
        "#     os.system(\"start output.mp3\")\n",
        "\n",
        "# if not answer_exp and  not answer_skill:\n",
        "#   text_to_speech(\"Sorry, couldn't extract the requested information from the CV.\")\n",
        "\n",
        "# if answer_exp:\n",
        "#   text_to_speech(answer_exp)\n",
        "\n",
        "# if answer_skill:\n",
        "#   text_to_speech(answer_skill)"
      ],
      "metadata": {
        "id": "-A8to2p99kQv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}